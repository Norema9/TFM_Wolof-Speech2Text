{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==2.1.0\n",
    "# !pip install huggingface_hub==0.5.1\n",
    "# !pip install librosa==0.9.1\n",
    "# !pip install numpy==1.22.0\n",
    "# !pip install pandarallel==1.6.1\n",
    "# !pip install pandas==1.4.2\n",
    "# !pip install scikit_learn==1.0.2\n",
    "# !pip install tensorflow==2.8.1\n",
    "# !pip install toml==0.10.2\n",
    "# !pip install torch==1.7.1\n",
    "# !pip install tqdm==4.64.0\n",
    "# !pip install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import sys\n",
    "import os\n",
    "import toml\n",
    "import warnings\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "os.chdir(r\"E:\\IA\\WOLOF\\SPEECH_TO_TEXT\")\n",
    "# to add the path of the different on module\n",
    "sys.path.append(r'CODES\\\\ASR-Wav2vec-Finetune-main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "from time import gmtime, strftime\n",
    "from utils.utils import *\n",
    "from utils.metric import Metric\n",
    "from dataloader.dataset import DefaultCollate\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '4444'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size, timeout=datetime.timedelta(seconds=3600 * 5))\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size, config, resume, preload):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']=config[\"meta\"][\"device_ids\"]\n",
    "    os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'INFO'\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    pretrained_path = config[\"meta\"][\"pretrained_path\"]\n",
    "    epochs = config[\"meta\"][\"epochs\"]\n",
    "    gradient_accumulation_steps = config[\"meta\"][\"gradient_accumulation_steps\"]\n",
    "    use_amp = config[\"meta\"][\"use_amp\"]\n",
    "    max_clip_grad_norm = config[\"meta\"][\"max_clip_grad_norm\"]\n",
    "    save_dir =  os.path.join(config[\"meta\"][\"save_dir\"], config[\"meta\"]['name'] + '/checkpoints')\n",
    "    log_dir = os.path.join(config[\"meta\"][\"save_dir\"], config[\"meta\"]['name'] + '/log_dir')\n",
    "    \n",
    "    if rank == 0:\n",
    "        # Creatr dirs\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "            \n",
    "        # Store config file\n",
    "        config_name = strftime(\"%Y-%m-%d %H~%M~%S\", gmtime()).replace(' ', '_') + '.toml'\n",
    "        with open(os.path.join(config[\"meta\"][\"save_dir\"], config[\"meta\"]['name'] + '\\\\' + config_name), 'w+') as f:\n",
    "            toml.dump(config, f)\n",
    "            f.close()\n",
    "\n",
    "    # This should be needed to be reproducible https://discuss.pytorch.org/t/setting-seed-in-torch-ddp/126638\n",
    "    config[\"meta\"][\"seed\"] += rank\n",
    "    set_seed(config[\"meta\"][\"seed\"])\n",
    "    config['val_dataset']['args']['sr'] = config['meta']['sr']\n",
    "    config['train_dataset']['args']['sr'] = config['meta']['sr']\n",
    "\n",
    "    config['train_dataset']['args']['rank'] = rank\n",
    "    config['val_dataset']['args']['rank'] = rank\n",
    "\n",
    "    config[\"train_dataset\"][\"args\"][\"dist\"] = dist\n",
    "    config[\"val_dataset\"][\"args\"][\"dist\"] = dist\n",
    "\n",
    "    config[\"train_dataset\"][\"args\"][\"special_tokens\"] = config[\"special_tokens\"]\n",
    "    config[\"val_dataset\"][\"args\"][\"special_tokens\"] = config[\"special_tokens\"]\n",
    "\n",
    "    train_base_ds = initialize_module(config[\"train_dataset\"][\"path\"], args=config[\"train_dataset\"][\"args\"])\n",
    "    vocab_dict = train_base_ds.get_vocab_dict()\n",
    "    with open(r'CODES\\MODELS\\WAV2VEC2\\vocabs\\vocab.json', 'w+') as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "        f.close()\n",
    "    dist.barrier()\n",
    "    # Create processor\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(r'CODES\\MODELS\\WAV2VEC2\\vocabs\\vocab.json', \n",
    "                                    **config[\"special_tokens\"],\n",
    "                                    word_delimiter_token=\"|\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(pretrained_path)\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    default_collate = DefaultCollate(processor, config['meta']['sr'])\n",
    "\n",
    "    # Create train dataloader\n",
    "    train_ds = train_base_ds.get_data()\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_ds,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        **config[\"train_dataset\"][\"sampler\"]\n",
    "    )\n",
    "    train_dl = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        **config[\"train_dataset\"][\"dataloader\"],\n",
    "        sampler = train_sampler,\n",
    "        collate_fn=default_collate\n",
    "    )\n",
    "\n",
    "    # Create val dataloader\n",
    "    val_base_ds = initialize_module(config[\"val_dataset\"][\"path\"], args=config[\"val_dataset\"][\"args\"])\n",
    "    val_ds = val_base_ds.get_data()\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        val_ds,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        **config[\"val_dataset\"][\"sampler\"]\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        dataset=val_ds,\n",
    "        **config[\"val_dataset\"][\"dataloader\"],\n",
    "        sampler = val_sampler,\n",
    "        collate_fn=default_collate\n",
    "    )\n",
    "\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        pretrained_path, \n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer),\n",
    "        gradient_checkpointing=False\n",
    "    )\n",
    "    \n",
    "    # freeze the wav2vec feature encoder, if you have small dataset, this helps a lot\n",
    "    model.freeze_feature_encoder()\n",
    "    # DDP for multi-processing\n",
    "    model = DDP(model.to(rank), device_ids=[rank], find_unused_parameters=True)\n",
    "\n",
    "    # Set up metric, scheduler, optmizer\n",
    "    compute_metric = Metric(processor)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params = model.parameters(),\n",
    "        lr = config[\"optimizer\"][\"lr\"]\n",
    "    )\n",
    "    steps_per_epoch = (len(train_dl)//gradient_accumulation_steps) + (len(train_dl)%gradient_accumulation_steps != 0)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=config[\"scheduler\"][\"max_lr\"], \n",
    "        epochs=epochs, \n",
    "        steps_per_epoch = steps_per_epoch)\n",
    "\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"Number of training utterances: \", len(train_ds))\n",
    "        print(\"Number of validation utterances: \", len(val_ds))\n",
    "\n",
    "    trainer_class = initialize_module(config[\"trainer\"][\"path\"], initialize=False)\n",
    "    trainer = trainer_class(\n",
    "        dist = dist,\n",
    "        rank = rank,\n",
    "        n_gpus = world_size,\n",
    "        config = config,\n",
    "        resume = resume,\n",
    "        preload = preload,\n",
    "        epochs = epochs,\n",
    "        steps_per_epoch = steps_per_epoch,\n",
    "        model = model,\n",
    "        compute_metric = compute_metric,\n",
    "        processor = processor,\n",
    "        train_dl = train_dl,\n",
    "        val_dl = val_dl,\n",
    "        train_sampler = train_sampler,\n",
    "        val_sampler = val_sampler,\n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler,\n",
    "        save_dir = save_dir,\n",
    "        log_dir = log_dir,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        use_amp = use_amp,\n",
    "        max_clip_grad_norm = max_clip_grad_norm\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arguments_and_execute(args):\n",
    "    config = toml.load(args['config'])\n",
    "    n_gpus = len(config['meta'][\"device_ids\"].split(','))\n",
    "    main(0, n_gpus, config, args['resume'], args.get('preload'))\n",
    "    # mp.spawn(\n",
    "    #     main,\n",
    "    #     args = (n_gpus, config, args['resume'], args.get('preload')),\n",
    "    #     nprocs = n_gpus,\n",
    "    #     join = True\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4780d578b58a4864903ce2aa5126a990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4376), Label(value='0 / 4376'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'á': 27, 'ã': 28, 'é': 29, 'ë': 30, 'ñ': 31, 'ó': 32, 'õ': 33, 'ŋ': 34, 'а': 35, 'о': 36, 'р': 37, 'с': 38, 'у': 39, 'ё': 40, 'ẽ': 41, '️': 42, '|': 0, '<bos>': 43, '<eos>': 44, '<unk>': 45, '<pad>': 46}\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf7f03528c5499aaf57a9a6edbd4908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=769), Label(value='0 / 769'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCODES\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASR-Wav2vec-Finetune-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mconfig.toml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# or False depending on your needs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreload\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     }\n\u001b[1;32m----> 7\u001b[0m     process_arguments_and_execute(args)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mprocess_arguments_and_execute\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m toml\u001b[38;5;241m.\u001b[39mload(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m n_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m main(\u001b[38;5;241m0\u001b[39m, n_gpus, config, args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m], args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreload\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[5], line 100\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(rank, world_size, config, resume, preload)\u001b[0m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze_feature_encoder()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# DDP for multi-processing\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m model \u001b[38;5;241m=\u001b[39m DDP(model\u001b[38;5;241m.\u001b[39mto(rank), device_ids\u001b[38;5;241m=\u001b[39m[rank], find_unused_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Set up metric, scheduler, optmizer\u001b[39;00m\n\u001b[0;32m    103\u001b[0m compute_metric \u001b[38;5;241m=\u001b[39m Metric(processor)\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:2595\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2591\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2594\u001b[0m         )\n\u001b[1;32m-> 2595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\maron\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = {\n",
    "        'config': r'CODES\\ASR-Wav2vec-Finetune-main\\config.toml',\n",
    "        'resume': False,  # or False depending on your needs\n",
    "        'preload': None\n",
    "    }\n",
    "    process_arguments_and_execute(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
